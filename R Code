# Calf survey project

# Objectives:
# 1 -> Characterize herds based on early life management practices
# 2 -> Evaluate the relationship between management practices and longevity, production, and profitability


# Loading packages --------------------------------------------------------
extrafont::loadfonts(device = "win")

require(tidyverse)
require(tidylog)
require(mice)
require(fpc) # Cluster validation
require(caret)
require(doParallel)
require(doSNOW)
require(cowplot)
require(iml)
require(future)
require(future.callr)



# Helping functions -------------------------------------------------------

# Function to calculate the Mean arctangent absolute percentage error as proposed
# by Kim & Kim (2016) DOI: https://doi.org/10.1016/j.ijforecast.2015.12.003
MAAPE <- function(obs, pred){
  
  return(mean(atan(abs(obs - pred)/abs(obs))))
  
}


# Function to evaluate model predictions
ModelEvaluation <- function(models,
                            target.variable,
                            train.data,
                            validation.data,
                            model.name,
                            imputation) {
  
  # Training data set
  
  r2.train <- R2(pred = predict(models[[imputation]], 
                                train.data %>% 
                                  filter(.imp == imputation)),
                 obs = train.data %>% 
                   filter(.imp == imputation) %>% 
                   pull(target.variable),
                 formula = "corr")
  
  mae.train <- MAE(pred = predict(models[[imputation]], 
                                  train.data %>% 
                                    filter(.imp == imputation)),
                   obs = train.data %>% 
                     filter(.imp == imputation) %>% 
                     pull(target.variable))
  
  rmse.train <- RMSE(pred = predict(models[[imputation]], 
                                    train.data %>% 
                                      filter(.imp == imputation)),
                     obs = train.data %>% 
                       filter(.imp == imputation) %>% 
                       pull(target.variable))
  
  
  maape.train <- MAAPE(pred = predict(models[[imputation]], 
                                      train.data %>% 
                                        filter(.imp == imputation)),
                       obs = train.data %>% 
                         filter(.imp == imputation) %>% 
                         pull(target.variable)) * 100
  
  
  # Validation data set
  
  r2.val <- R2(pred = predict(models[[imputation]], 
                              validation.data %>% 
                                filter(.imp == imputation)),
               obs = validation.data %>% 
                 filter(.imp == imputation) %>% 
                 pull(target.variable),
               formula = "corr")
  
  mae.val <- MAE(pred = predict(models[[imputation]], 
                                validation.data %>% 
                                  filter(.imp == imputation)),
                 obs = validation.data %>% 
                   filter(.imp == imputation) %>% 
                   pull(target.variable))
  
  rmse.val <- RMSE(pred = predict(models[[imputation]], 
                                  validation.data %>% 
                                    filter(.imp == imputation)),
                   obs = validation.data %>% 
                     filter(.imp == imputation) %>% 
                     pull(target.variable))
  
  maape.val <- MAAPE(pred = predict(models[[imputation]], 
                                    validation.data %>% 
                                      filter(.imp == imputation)),
                     obs = validation.data %>% 
                       filter(.imp == imputation) %>% 
                       pull(target.variable)) * 100
  
  
  res <- tibble(imputation = imputation,
                model = model.name,
                data.set = c(rep("training", 4), rep("validation", 4)),
                metric = rep(c("r2", "MAE", "RMSE", "MAAPE"), 2),
                value = c(r2.train, mae.train, rmse.train, maape.train,
                          r2.val, mae.val, rmse.val, maape.val))
  
  return(res)
  
}



# Reading data ------------------------------------------------------------

pa <- readRDS(paste(data.path, "proAction_anon_hrd_tbl.rds", sep = ""))
lpl <- readRDS(paste(data.path, "prod_lifetime_anon_anm_tbl.rds", sep = ""))
research_data <- readRDS(paste(data.path, "res_data_anon_anm_tbl.rds", sep = ""))
survey <- readRDS(paste(data.path, "survey_calf_feed_anon_tbl.rds", sep = ""))
hsi <- readRDS(paste(data.path, "whi_indicators_anon_tbl.rds", sep = ""))


# Creating a more readable herd ID

unique_hrd_id <- pa %>% 
  select(id) %>% 
  drop_na() %>% 
  rbind(lpl %>% select(id),
        research_data %>% select(id),
        survey %>% select(id),
        hsi %>%  select(id)) %>% 
  distinct(id) %>% 
  mutate(hrd_id = seq_along(id))



# Preparing data ----------------------------------------------------------

## 3+ lactation data ####

hsi1 <- hsi %>% 
  
  # Assigning a more readable herd ID
  drop_na(id) %>% 
  left_join(unique_hrd_id, by = "id") %>% 
  select(hrd_id, setdiff(names(hsi), c("id"))) %>% 
  
  
  # Keeping only the last 12 months starting at the proAction evaluation
  mutate(period = as.numeric(as.Date(visit_date, format = "%V-%m-%d") - 
                               as.Date(test_date, format = "%V-%m-%d"))) %>%
  filter(period <= 365.25) %>% 
  
  
  # Keeping only herds with at least 3 tests over the 12 month period
  filter(cnttests >= 3) %>% 
  

  # Calculating a final average per herd
  select(hrd_id, avg_pcntg_lgvt) %>% 
  group_by(hrd_id) %>% 
  summarise_if(is.numeric, mean, na.rm = T) %>% 
  ungroup()




# Getting count of animals

hsi %>% 
  
  # Assigning a more readable herd ID
  drop_na(id) %>% 
  left_join(unique_hrd_id, by = "id") %>% 
  select(hrd_id, setdiff(names(hsi), c("id"))) %>% 
  
  
  # Keeping only the last 12 months starting at the proAction evaluation
  mutate(period = as.numeric(as.Date(visit_date, format = "%V-%m-%d") - 
                               as.Date(test_date, format = "%V-%m-%d"))) %>%
  filter(period <= 365.25) %>% 
  drop_na(avg_pcntg_lgvt) %>%
  count()


# Getting count of unique herds

hsi %>% 
  
  # Assigning a more readable herd ID
  drop_na(id) %>% 
  left_join(unique_hrd_id, by = "id") %>% 
  select(hrd_id, setdiff(names(hsi), c("id"))) %>% 
  
  
  # Keeping only the last 12 months starting at the proAction evaluation
  mutate(period = as.numeric(as.Date(visit_date, format = "%V-%m-%d") - 
                               as.Date(test_date, format = "%V-%m-%d"))) %>%
  filter(period <= 365.25) %>% 
  drop_na(avg_pcntg_lgvt) %>%
  distinct(hrd_id) %>% 
  count()




## Length of productive life data ####

lpl1 <- lpl %>% 
  
  # Assigning a more readable herd ID
  drop_na(id) %>% 
  left_join(unique_hrd_id, by = "id") %>% 
  
  # Keeping only data from animals that left the herd in the last 12 months prior to 
  # the proAction evaluation
  mutate(period = as.numeric(as.Date(visit_date, format = "%V-%m-%d") - 
                               as.Date(left_herd_date, format = "%V-%m-%d"))) %>%
  filter(period <= 365.25) %>% 
  
  # Remove negative values on LPL
  filter(prod_lt > 0) %>% 
  
  # Convert LPL to year
  mutate(lpl = prod_lt/365.25) %>% 
  select(hrd_id, lpl) %>% 
  
  group_by(hrd_id) %>% 
  summarise(lpl = mean(lpl))



# Getting count of animals

lpl %>% 
  
  # Assigning a more readable herd ID
  drop_na(id) %>% 
  left_join(unique_hrd_id, by = "id") %>% 
  
  # Keeping only data from animals that left the herd in the last 12 months prior to 
  # the proAction evaluation
  mutate(period = as.numeric(as.Date(visit_date, format = "%V-%m-%d") - 
                               as.Date(left_herd_date, format = "%V-%m-%d"))) %>%
  filter(period <= 365.25) %>% 
  drop_na(prod_lt) %>%
  count()


# Getting count of unique herds

lpl %>% 
  
  # Assigning a more readable herd ID
  drop_na(id) %>% 
  left_join(unique_hrd_id, by = "id") %>% 
  
  # Keeping only data from animals that left the herd in the last 12 months prior to 
  # the proAction evaluation
  mutate(period = as.numeric(as.Date(visit_date, format = "%V-%m-%d") - 
                               as.Date(left_herd_date, format = "%V-%m-%d"))) %>%
  filter(period <= 365.25) %>% 
  drop_na(prod_lt) %>%
  distinct(hrd_id) %>% 
  count()




## Production data ####

# On research data, production and economics are provided by lactation
# Some animals moved between herds. For those animals, I am keeping
# production to the herd in which the cow finished her lactation

research_data1 <- research_data %>% 
  
  # Assigning a more readable herd ID
  drop_na(id) %>% 
  left_join(unique_hrd_id, by = "id") %>% 
  
  # Keeping only data from animals that finished the lactation in the last 12 months prior to 
  # the proAction evaluation
  mutate(period = as.numeric(as.Date(visit_date, format = "%V-%m-%d") - 
                               as.Date(lct_end_date, format = "%V-%m-%d"))) %>%
  filter(period <= 365.25) %>% 
  
  
  # Removing NAs on production variables because I need complete 
  # obs to calculate ECM. Also, removing production values of ZERO
  
  # There are also some Zeros and NAs on milk value, despite having info
  # on production. Removing this observations as well.
  
  drop_na(c(lact_date_yld_milk, lact_date_yld_fat,
            lact_date_yld_prot, cumul_milk_value)) %>% 
  
  filter(lact_date_yld_milk != 0,
         lact_date_yld_fat != 0,
         lact_date_yld_prot != 0,
         cumul_milk_value != 0) %>% 
  
  select(hrd_id, anm_id_anon, lact_date_yld_milk, lact_date_yld_fat, 
         lact_date_yld_prot, cumul_milk_value) %>% 
  
  # Calculate ECM
  mutate(ecm = 12.55*lact_date_yld_fat + 
           7.39*lact_date_yld_prot + 
           0.2595*lact_date_yld_milk) %>%
  
  # Calculate cumulative sum per animal in each herd (necessary because some animals
  # moved between herds)
  group_by(hrd_id, anm_id_anon) %>% 
  summarise(across(everything(), sum)) %>% 
  
  
  # Calculate cumulative herd average
  select(-anm_id_anon) %>% 
  group_by(hrd_id) %>% 
  summarise_if(is.numeric, mean)






# Getting count of animals

research_data %>% 
  
  # Assigning a more readable herd ID
  drop_na(id) %>% 
  left_join(unique_hrd_id, by = "id") %>% 
  
  # Keeping only data from animals that finished the lactation in the last 12 months prior to 
  # the proAction evaluation
  mutate(period = as.numeric(as.Date(visit_date, format = "%V-%m-%d") - 
                               as.Date(lct_end_date, format = "%V-%m-%d"))) %>%
  filter(period <= 365.25) %>% 
  distinct(anm_id_anon) %>%
  count()


# Getting count of unique herds

research_data %>% 
  
  # Assigning a more readable herd ID
  drop_na(id) %>% 
  left_join(unique_hrd_id, by = "id") %>% 
  
  # Keeping only data from animals that finished the lactation in the last 12 months prior to 
  # the proAction evaluation
  mutate(period = as.numeric(as.Date(visit_date, format = "%V-%m-%d") - 
                               as.Date(lct_end_date, format = "%V-%m-%d"))) %>%
  filter(period <= 365.25) %>% 
  distinct(hrd_id) %>% 
  count()







## Calf survey data ####
sv <- survey %>%
  
  # Assigning a more readable herd ID
  drop_na(id) %>% 
  left_join(unique_hrd_id, by = "id") %>% 
  select(hrd_id, setdiff(names(survey), c("id"))) %>% 
  
  # Recorded date is in a format that does not work well with R
  mutate(recorded_date = lubridate::date(recorded_date)) %>% 
  mutate_if(is.character, as.factor) %>% 
  
  # There are some empty cells, converting those to NA
  na_if("") %>% 
  droplevels() %>% 
  
  # removing repeated variables
  select(-crh_cd, -fch_cd)



# Exploring calf survey data
summary(sv)

# Exploring and potentially fixing missing observations
DataExplorer::plot_missing(sv)


# First round of cleaning

sv1 <- sv %>%  
  
  # Removing observations on Organic farms
  filter(regime == "Conventional") %>% 
  
  # Only interested in female calves. Therefore, removing the variables describing
  # how male calves are treated differently than female.
  select(-c(gender_management_ind, calf_management_reasons_1, calf_management_reasons_2,
            calf_management_reasons_3, calf_management_reasons_4, calf_management_reasons_5)) %>%
  
  # There are too many missing observations on how the farmer check the criteria used for first
  # breeding and weaning (estimated or measured/weighed). Removing this variables
  select(-c(first_brdng_msrmnt_mthds, weaning_msrmnt_mthds)) %>%
  
  # There are also a considerable number of missing data on the amount of Fat and Protein
  # of the milk replacer. Though the majority, not all farms fed milk replacer to calves.
  # Imputing zero for those farms that did not and seeing if this reduce the amount of NAs
  mutate(milk_replacer_fat_pcnt = ifelse(!is.na(calf_milk_sources) & 
                                           calf_milk_sources != "Powdered milk replacer", 0,
                                         milk_replacer_fat_pcnt),
         milk_replacer_prt_pcnt = ifelse(!is.na(calf_milk_sources) & 
                                           calf_milk_sources != "Powdered milk replacer", 0,
                                         milk_replacer_prt_pcnt)) %>% 
  
  
  # Farms that do not use milk replacer are not differentiated on the question whether milk replacer
  # is medicated.Changing that
  mutate(milk_replacer_medicated_ind = as.character(milk_replacer_medicated_ind)) %>% 
  mutate(milk_replacer_medicated_ind = ifelse(!is.na(calf_milk_sources) & 
                                           calf_milk_sources != "Powdered milk replacer", 
                                           "No milk replacer", milk_replacer_medicated_ind)) %>% 
  
  # There are farms which responded more than one type of bedding. Combining both into a single
  # variable 
  unite(col = calf_bedding_type, 
        calf_bedding_type_1, calf_bedding_type_2,
        na.rm = TRUE, remove = TRUE) %>% 
  
  
  # Colostrum form was only asked to farmers who do not use powdered colostrum
  mutate(colostrum_source_forms = as.character(colostrum_source_forms)) %>%
  mutate(colostrum_source_forms = ifelse(colostrum_sources == "Powdered colostrum",
                                           "Not applicable", 
                                         colostrum_source_forms)) %>%  
  
  # There are some empty cells, converting those to NA
  na_if("") %>% 
  
  # The variable frequency of adding bedding is on a lot of different units (per day, per week,
  # and per month). Converting it to only months
  
  # Day to month = Frequency * 365.25 (days in the year) divided by 12 months
  # Week to month = Frequency * (365.25/7) divided by 12 months
  mutate(bdng_added_fqcy = ifelse(frequency_units == "month",
                                  bdng_added_fqcy, 
                                  ifelse(frequency_units == "week",
                                         (bdng_added_fqcy*(365.25/7))/12, 
                                         ifelse(frequency_units == "day",
                                                (bdng_added_fqcy*365.25)/12, 
                                                bdng_added_fqcy)))) %>% 
  select(-frequency_units) %>% 
  mutate_if(is.character, as.factor)





sv1 %>% 
  DataExplorer::plot_missing()


sv1 %>% 
  DataExplorer::plot_bar()



sv2 <- sv1 %>% 
  mutate_if(is.factor, as.character) %>% 
  mutate(systraite = ifelse(systraite == "4. N/A", NA, systraite),
         # There are too little farms with first colostrum from 7 to more than 12 hrs. Since colostrum is
         # best only if fed in the first 6 hours, I am combining the least frequent levels
         
         first_colostrum_hrs = ifelse(first_colostrum_hrs %in% c("From 7 to 12hrs", 
                                                                 "More than 12hrs"),
                                      "More than 6hrs", first_colostrum_hrs),
         # calf_milk_feed_sys has too many levels with very little responses. Combining the least
         # common answers to level "other"
         calf_milk_feed_sys = ifelse(is.na(calf_milk_feed_sys), NA,
                                     ifelse(!(calf_milk_feed_sys %in% c("Individual bucket with teats",
                                                                        "Individual bucket without teats",
                                                                        "Bottle",
                                                                        "Automatique feeding system",
                                                                        "Feed line (free feed)")),
                                            "other", calf_milk_feed_sys)),
         
         # Very few farms answered weaning the animals "From 71 to 90" and "More than 90". Combining these
         # answers to a new level "More than 70" 
         weaning_age_days = ifelse(is.na(weaning_age_days), NA,
                                   ifelse(weaning_age_days %in% c("From 71 to 90", "More than 90"),
                                          "More than 70", weaning_age_days)),
         
         # Only 9 farms said Non pasturized / Acidified or Pasteurized / Acidified.
         # Grouping it into other
         calf_milk_source_forms = ifelse(calf_milk_source_forms  %in% 
                                           c("Non pasturized / Acidified", 
                                             "Pasteurized / Acidified"),
                                         "other", calf_milk_source_forms)) %>% 
  
  
  # Removing 1 herd that said not feeding colostrum to calves
  filter(colostrum_sys != "No colostrum offered") %>% 
  
  # Two farms said 0 litres of colostrom, though it was a mistake since they said using a cow
  # or a bucket to feed colostrum. Replacing it for missing data
  mutate(first_colostrum_liters = ifelse(first_colostrum_liters == "0 litre",
                                         NA, first_colostrum_liters)) %>% 
  
  
  mutate_if(is.character, factor) %>% 
  mutate_at(c("hrd_id"), factor) %>% 
  mutate_at("starter_feed_prot_pcnts", as.character) %>% 
  mutate_at("starter_feed_prot_pcnts", as.numeric) %>% 
  droplevels()


summary(sv2)

# Checking categorical variables
sv2 %>% 
  DataExplorer::plot_bar()



# Looking at the numerical variables
sv2 %>% 
  DataExplorer::plot_histogram()


# There seem to be a problem with bedding change frequency. Some farms answered adding bedding
# 3, 4, and 7 time per day, which seems a little too much! It could be a mistake in selecting the 
# correct frequency unit (i.e., maybe they meant 7 times per week for example) or it could be that 
# they check on calves multiple times per day and they add bedding if it is needed. Since there is 
# no way a could confirm that, I will leave that variable like that.


# Looking at correlations

DataExplorer::plot_correlation(sv2,
                               type = "continuous",
                               cor_args = list(use = "pairwise.complete.obs"))

DataExplorer::plot_correlation(sv2 %>% 
                                 select(-c(hrd_id, systraite, regime, # only conventional 
                                           recorded_date)),
                               type = "discrete",
                               cor_args = list(use = "pairwise.complete.obs"))


sv3 <- sv2 %>% 
  
  # colostrum and milk meals are EXACTLY the same! Combining both
  select(-milk_mealsperday) %>%
  rename(colost_AND_milk_mealsperday = colostrum_mealsperday) %>% 
  
  # Some variables are ordered (e.g., Amount of colostrum). Format this variables
  # correctly
  mutate(calf_removal_hrs = factor(calf_removal_hrs,
                                   levels = c("Less than 1hr", "1 to 6hrs", "7 to 12hrs", "13 to 24hrs",
                                              "More than 24hrs"),
                                   ordered = TRUE),
         first_colostrum_hrs = factor(first_colostrum_hrs,
                                      levels = c("Less than 1h after birth", "From 1 to 6hrs",
                                                 "More than 6hrs"),
                                      ordered = TRUE),
         colost_AND_milk_mealsperday = factor(colost_AND_milk_mealsperday,
                                              levels = c("1", "2", "3"),
                                              ordered = TRUE),
         first_colostrum_liters = factor(first_colostrum_liters,
                                         levels = c("1 litre", "2 litres", 
                                                    "3 litres", "4 litres or more"),
                                         ordered = TRUE),
         subs_colostrum_liters = factor(subs_colostrum_liters,
                                         levels = c("0 litre", "1 litre", "2 litres", 
                                                    "3 litres", "4 litres or more"),
                                         ordered = TRUE),
         weaning_age_days = factor(weaning_age_days,
                                   levels = c("Less than 50", "From 51 to 70",
                                              "More than 70"),
                                   ordered = TRUE))
  

sv3 %>% 
  DataExplorer::plot_missing()

DataExplorer::plot_correlation(sv3 %>% 
                                 select(-c(hrd_id, systraite, regime, # only conventional 
                                           recorded_date)),
                               type = "discrete",
                               cor_args = list(use = "pairwise.complete.obs"))



# Clustering early life management practices  -------------------------
# Description of early life management practices

# Using the Gower distance because there are both categorical and numerical variables
# Also, using the daisy function from the cluster package because it accept NA


clust.dist <- cluster::daisy(sv3 %>%
                               select(-c(hrd_id, regime, # only conventional 
                                                recorded_date, systraite)),
                             metric = "gower")

# Avaliando a dispersão das parcelas perdidas

n.NA <- sv3 %>%
  select(-c(hrd_id, regime, # only conventional 
            recorded_date, systraite)) %>% 
  summarise_all(~sum(is.na(.))) %>% 
  reshape2::melt(value.name = "n.NA")

n.NON_NA <- sv3 %>%
  select(-c(hrd_id, regime, # only conventional 
            recorded_date, systraite)) %>% 
  summarise_all(~sum(!is.na(.))) %>% 
  reshape2::melt(value.name = "non.NA")




n.NA %>%
  left_join(n.NON_NA, by = "variable") %>%
  mutate(percNA = round((n.NA/as.integer(count(sv3)))*100, 1)) %>% View()




## Cluster Validation ####

# Jaccard's bootstrap distance
# Hierarchical with Ward method

valid.ward <- data.frame(NULL)


for(i in 2:7) {
  
  cbv <- clusterboot(data = as.dist(clust.dist),
                     distances = TRUE,
                     B = 100,
                     bootmethod = "boot",
                     clustermethod = disthclustCBI,
                     noisemethod = FALSE,
                     k = i,
                     method = "ward.D2",
                     seed = 1801,
                     count = FALSE)
  
  
  valid.ward <- valid.ward %>% 
    rbind(data.frame(n_clusters = rep(cbv$nc, length(cbv$bootmean)),
                     cluster_ID = c(1:length(cbv$bootmean)),
                     jacard_boot = cbv$bootmean))
  
  print(
    paste(i, " cluster finished. Only ", 7-i, " missing :)",
          sep = "")
  )
  
}


# Partitioning around medoids

valid.pam <- data.frame(NULL)

for(i in 2:7){
  
  cbv <- clusterboot(data = as.dist(clust.dist),
                     distances = TRUE,
                     B = 100,
                     bootmethod = "boot",
                     clustermethod = claraCBI,
                     noisemethod = FALSE,
                     k = i,
                     usepam = TRUE,
                     seed = 1801,
                     count = FALSE)
  
  valid.pam <- valid.pam %>% 
    rbind(data.frame(n_clusters = rep(cbv$nc, length(cbv$bootmean)),
                     cluster_ID = c(1:length(cbv$bootmean)),
                     jacard_boot = cbv$bootmean))
  
  print(
    paste(i, " cluster finished. Only ", 7-i, " missing :)",
          sep = "")
  )
  
}


# normal mixture model

valid.mclust <- data.frame(NULL) 


for(i in 2:7){
  
  cbv <- clusterboot(data = as.dist(clust.dist),
                     distances = TRUE,
                     B = 100,
                     bootmethod = "boot",
                     clustermethod = distnoisemclustCBI,
                     noisemethod = FALSE,
                     k = i,
                     seed = 1801,
                     count = FALSE)
  
  valid.mclust <- valid.mclust %>% 
    rbind(data.frame(n_clusters = rep(cbv$nc, length(cbv$bootmean)),
                     cluster_ID = c(1:length(cbv$bootmean)),
                     jacard_boot = cbv$bootmean))
  
  print(
    paste(i, " cluster finished. Only ", 7-i, " missing :)",
          sep = "")
  )
  
}




valid.ward %>% 
  group_by(n_clusters) %>% 
  summarise(avg_jacard = mean(jacard_boot)) %>% 
  mutate_all(round, 3) %>% 
  rhandsontable::rhandsontable(useTypes = FALSE)

valid.ward %>% 
  mutate_all(round, 2) %>% 
  rhandsontable::rhandsontable(useTypes = FALSE)




valid.pam %>% 
  group_by(n_clusters) %>% 
  summarise(avg_jacard = mean(jacard_boot)) %>% 
  mutate_all(round, 3) %>% 
  rhandsontable::rhandsontable(useTypes = FALSE)

valid.pam %>% 
  mutate_all(round, 2) %>% 
  rhandsontable::rhandsontable(useTypes = FALSE)




valid.mclust %>% 
  group_by(n_clusters) %>% 
  summarise(avg_jacard = mean(jacard_boot)) %>% 
  mutate_all(round, 3) %>% 
  rhandsontable::rhandsontable(useTypes = FALSE)

valid.mclust %>%  
  mutate_all(round, 2) %>% 
  rhandsontable::rhandsontable(useTypes = FALSE)


# Normal mixture model-based with 2 clusters was the best option!


## Clustering ####

set.seed(1801)
sv.cluster <- distnoisemclustCBI(dmatrix = as.dist(clust.dist),
                                 k = 2,
                                 diss = TRUE)



# Visualizing the clusters using MDS
cluster_mds <- cmdscale(d = as.dist(clust.dist),
                        k = 2, # First two dimensions
                        eig = TRUE)




# Visualizing the cluster results on nlPCA loads

ggplot(mapping = aes(x = cluster_mds$points[,1],
                     y = cluster_mds$points[,2],
                     color = factor(sv.cluster$partition),
                     shape = factor(sv.cluster$partition))) +
  geom_point(alpha = 0.6, size = 1.3) +
  scale_color_manual(values = c("#1F78B4", "#FF7F00")) +
  
  theme_bw(base_family = "Times New Roman") +
  labs(x = "MDS1",
       y = "MDS2",
       shape = "Cluster",
       colour = "Cluster") +

  theme(axis.text = element_text(size = 10, color = "black"),
        legend.text = element_text(size = 10, family = "Times New Roman"),
        legend.position = "right", #c(0.91, 0.812),
        # legend.backgr→ound = element_rect(fill = "white", color = "black"),
        legend.justification="center",
        axis.title.x = element_text(size = 10, 
                                    margin = margin(t = 10, # top
                                                    r = 0, # right 
                                                    b = 0, # bottom
                                                    l = 0)),  # left
        axis.title.y = element_text(size = 10,
                                    margin = margin(t = 0, # top
                                                    r = 10, # right 
                                                    b = 0, # bottom
                                                    l = 0)),
        legend.title.align = 0.5)



## Describing clusters ####
# In order to describe the clusters, I will be using a simpler methodology.
# I will use the v-test from the FactoMineR package. It provides a straight 
# forward way to identify which variables clearly describe a cluster compared to the 
# population. However, it does not handle NA. Using the mice package to impute 
# the missing variables before getting cluster descriptions 


sv.imputed <- mice(data = sv3 %>%
                     select(-c(hrd_id, regime, # only conventional 
                               recorded_date, systraite)) %>% 
                     mutate(cluster = factor(sv.cluster$partition)), 
                   method = "rf",
                   seed = 1801,
                   m = 1,
                   printFlag = TRUE)


sv.imputed %>% 
  mice::complete("long") %>%
  summary()

densityplot(sv.imputed)


sv.cluster.desc <- FactoMineR::catdes(donnee = sv.imputed %>% 
                                        mice::complete("long") %>% 
                                        select(-.id, -.imp),
                                      num.var = 34,
                                      proba = 0.05)


# Cluster 1

sv.cluster.desc$category$`1` %>%
  data.frame() %>%
  rownames_to_column("variable") %>%
  arrange(desc(v.test)) %>%
  mutate_if(is.numeric, round, 1) %>%
  head(10) %>%  
  rhandsontable::rhandsontable(useTypes = FALSE)



sv.cluster.desc$quanti$`1` %>%
  data.frame() %>%
  rownames_to_column("variable") %>%
  arrange(desc(v.test)) %>%
  mutate_if(is.numeric, round, 2) %>%
  head(10) %>%  
  rhandsontable::rhandsontable(useTypes = FALSE)




# Cluster 2
sv.cluster.desc$category$`2` %>%
  data.frame() %>%
  rownames_to_column("variable") %>%
  arrange(desc(v.test)) %>%
  mutate_if(is.numeric, round, 1) %>%
  head(10) %>%  
  rhandsontable::rhandsontable(useTypes = FALSE)


sv.cluster.desc$quanti$`2` %>%
  data.frame() %>%
  rownames_to_column("variable") %>%
  arrange(v.test) %>%
  mutate_if(is.numeric, round, 1) %>%
  head(10) %>%  
  rhandsontable::rhandsontable(useTypes = FALSE)






# Early life and longevity ------------------------------------------------

# Creating data file

wd1 <- sv3 %>% 
  
  mutate(cluster = factor(sv.cluster$partition), 
         hrd_id = as.numeric(as.character(hrd_id))) %>% 
  left_join(research_data1, by = "hrd_id") %>% 
  left_join(hsi1 %>% 
              select(hrd_id, avg_pcntg_lgvt),
            by = "hrd_id") %>% 
  rename(lact3plus = avg_pcntg_lgvt) %>% 
  left_join(lpl1,
            by = "hrd_id") %>%
  drop_na(cumul_milk_value, ecm, lact3plus, lpl)

summary(wd1)


wd2 <- wd1 %>% 
  
  # Some levels on colostrum system, colostrum source, and colostrum_source_forms 
  # too low to use in machine learning. Therefore, I will remove those
  filter(colostrum_sys != "Other",
         colostrum_sources != "Other",
         colostrum_source_forms != "Frozen pasteurized") %>%
  droplevels()

summary(wd2)


wd2 %>% 
  select(lpl, lact3plus, ecm, cumul_milk_value) %>% 
  summary()


# Exploring missing data
n.NA.wd <- wd2 %>%
  select(-c(hrd_id, regime, # only conventional 
            recorded_date, systraite)) %>% 
  summarise_all(~sum(is.na(.))) %>% 
  reshape2::melt(value.name = "n.NA")


n.NON_NA.wd <- wd2 %>%
  select(-c(hrd_id, regime, # only conventional 
            recorded_date, systraite)) %>% 
  summarise_all(~sum(!is.na(.))) %>% 
  reshape2::melt(value.name = "non.NA")




n.NA.wd %>%
  left_join(n.NON_NA.wd, by = "variable") %>%
  mutate(percNA = round((n.NA/as.integer(count(wd2)))*100, 1)) %>% View()



# Exploring missing observations
DataExplorer::plot_missing(wd2)
summary(wd2)


  
ggplot(wd2, aes(x = cluster, y = ecm)) +
  geom_boxplot()

ggplot(wd2, aes(x = cluster, y = cumul_milk_value)) +
  geom_boxplot()


ggplot(wd2, aes(x = cluster, y = lact3plus)) +
  geom_boxplot()

ggplot(wd2, aes(x = cluster, y = lpl)) +
  geom_boxplot()



## Multiple imputation  ####

# mice:::find.collinear(wd2)

wd.imputed <- mice(data = wd2 %>%
                     select(-c(hrd_id, systraite, regime, # only conventional 
                               recorded_date)), 
                   method = "rf",
                   seed = 1801,
                   m = 10,
                   printFlag = TRUE)


# inspect quality of imputations
plot(wd.imputed)
densityplot(wd.imputed)

wd.imputed %>% 
  mice::complete("long") %>% 
  summary()




# Length of productive life
wd.lpl.imputed.stats <- wd.imputed %>% 
  mice::complete("long") %>%
  
  # Selecting variables based on top variables that described the clusters
  select(c(.imp, lpl, 
           
           # Categorical variables
           "milk_replacer_medicated_ind", "calf_milk_source_forms", "calf_milk_sources",
           "calf_milk_feed_sys", "noweaned_housing_groupings", "noweaned_housing_details",
           "weaned_housing_groupings", "weaned_housing_details", "igg_conc_method",
           "colostrum_sys",
           
           # Numeric variables
           "first_concentrate_age_days", "bdng_added_fqcy", "first_water_age_days", 
           "starter_feed_prot_pcnts", "daily_feeding_amnt", "milk_replacer_fat_pcnt", 
           "milk_replacer_prt_pcnt"))


# Percentage of cows on 3rd or greater lactations
wd.lact3plus.imputed.stats <- wd.imputed %>% 
  mice::complete("long") %>% 
  
  # Selecting variables based on top variables that described the clusters
  select(c(.imp, lact3plus, 
           
           # Categorical variables
           "milk_replacer_medicated_ind", "calf_milk_source_forms", "calf_milk_sources",
           "calf_milk_feed_sys", "noweaned_housing_groupings", "noweaned_housing_details",
           "weaned_housing_groupings", "weaned_housing_details", "igg_conc_method",
           "colostrum_sys",
           
           # Numeric variables
           "first_concentrate_age_days", "bdng_added_fqcy", "first_water_age_days", 
           "starter_feed_prot_pcnts", "daily_feeding_amnt", "milk_replacer_fat_pcnt", 
           "milk_replacer_prt_pcnt"))



# Energy-corrected milk
wd.ecm.imputed.stats <- wd.imputed %>% 
  mice::complete("long") %>% 
  
  # Selecting variables based on top variables that described the clusters
  select(c(.imp, ecm, 
           
           # Categorical variables
           "milk_replacer_medicated_ind", "calf_milk_source_forms", "calf_milk_sources",
           "calf_milk_feed_sys", "noweaned_housing_groupings", "noweaned_housing_details",
           "weaned_housing_groupings", "weaned_housing_details", "igg_conc_method",
           "colostrum_sys",
           
           # Numeric variables
           "first_concentrate_age_days", "bdng_added_fqcy", "first_water_age_days", 
           "starter_feed_prot_pcnts", "daily_feeding_amnt", "milk_replacer_fat_pcnt", 
           "milk_replacer_prt_pcnt"))



# Milk value
wd.cumul_milk_value.imputed.stats <- wd.imputed %>% 
  mice::complete("long") %>% 
  
  # Selecting variables based on top variables that described the clusters
  select(c(.imp, cumul_milk_value, 
           
           # Categorical variables
           "milk_replacer_medicated_ind", "calf_milk_source_forms", "calf_milk_sources",
           "calf_milk_feed_sys", "noweaned_housing_groupings", "noweaned_housing_details",
           "weaned_housing_groupings", "weaned_housing_details", "igg_conc_method",
           "colostrum_sys",
           
           # Numeric variables
           "first_concentrate_age_days", "bdng_added_fqcy", "first_water_age_days", 
           "starter_feed_prot_pcnts", "daily_feeding_amnt", "milk_replacer_fat_pcnt", 
           "milk_replacer_prt_pcnt"))


# Length of productive Life ------------------------------------------------

# Exploratory graphs
wd.lpl.imputed.stats %>% 
  DataExplorer::plot_bar()

wd.lpl.imputed.stats %>% 
  DataExplorer::plot_histogram()

wd.lpl.imputed.stats %>% 
  DataExplorer::plot_correlation(type = "continuous")

wd.lpl.imputed.stats %>% 
  DataExplorer::plot_correlation(type = "discrete")



## Splitting data ####
wd.lpl.imput.train.split <- data.frame()
wd.lpl.imput.val.split <- data.frame()

for (i in 1:10) {
  
  data <- wd.lpl.imputed.stats %>%
    filter(.imp == i)
  
  splitIndex <- splitTools::partition(data$lpl,
                                      p = c(train = 0.80, valid = 0.20),
                                      seed = 1801)
  
  wd.lpl.imput.train.split <- rbind(wd.lpl.imput.train.split,
                                    data[splitIndex$train,])
  
  wd.lpl.imput.val.split <- rbind(wd.lpl.imput.val.split,
                                  data[splitIndex$valid,])
  
}


# Inspecting it
summary(wd.lpl.imput.train.split %>% 
          filter(.imp == 1))


summary(wd.lpl.imput.val.split %>% 
          filter(.imp == 1))



## Sanity check ####
# Training a classic mixed effect model for each of the imputed files


r2.lpl.lm <- c()
rmse.lpl.lm <- c()

for(i in 1:10) {
  
  lpl.lm <- lm(lpl ~ .,
               data = wd.lpl.imput.train.split %>% 
                 filter(.imp == i) %>% 
                 select(-.imp))
  
  anova(lpl.lm) %>% print()
  
  
  r2.lpl.lm[i] <- caret::R2(pred = predict(lpl.lm,
                                           wd.lpl.imput.val.split %>% 
                                             filter(.imp == i)),
                            obs = wd.lpl.imput.val.split %>% 
                              filter(.imp == i) %>% 
                              pull(lpl),
                            formula = "corr")
  
  rmse.lpl.lm[i] <- caret::RMSE(pred = predict(lpl.lm,
                                               wd.lpl.imput.val.split %>% 
                                                 filter(.imp == i)),
                                obs = wd.lpl.imput.val.split %>% 
                                  filter(.imp == i) %>% 
                                  pull(lpl))
  
}


mean(r2.lpl.lm) %>% 
  round(3)

mean(rmse.lpl.lm) %>% 
  round(3)




## Training machine learning models ####

# Generating seeds for the random processes
number <- 5
repeats <- 10


set.seed(1801)
seeds <- vector(mode = "list", length = (number*repeats+1))
for(i in 1:(number*repeats)) seeds[[i]] <- sample.int(1000000, 27)

# For the last model:
seeds[[(number*repeats+1)]] <- sample.int(1000000, 1)


set.seed(1801)
fit_control <- trainControl(method = "adaptive_cv",
                            search = "grid",
                            number = number,
                            repeats = repeats,
                            adaptive = list(min = 5, alpha = 0.05, 
                                            method = "gls", 
                                            complete = TRUE),
                            allowParallel = TRUE,
                            verboseIter = FALSE,
                            seeds = seeds)

# Setting parallel clusters

cl <- makePSOCKcluster(parallel::detectCores())
registerDoParallel(cl, cores = parallel::detectCores())

getDoParWorkers()



### Classification and regression decision Tree ####

lpl.tree.models <- list()


for(i in 1:10) {
  
  
  data <- wd.lpl.imput.train.split %>% 
    filter(.imp == i) %>% 
    select(-.imp) %>% 
    as.data.frame()
  
  
  set.seed(1801)
  tree_lpl <- train(lpl ~ ., 
                    data = data, 
                    method = "rpart",
                    trControl = fit_control)
  
  lpl.tree.models[[i]] <- tree_lpl
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}



### Gradient boosting machine ####

lpl.gbm.models <- list()


for(i in 1:10) {
  
  
  data <- wd.lpl.imput.train.split %>% 
    filter(.imp == i) %>% 
    select(-.imp) %>% 
    as.data.frame()
  
  
  set.seed(1801)
  gbm_lpl <- train(lpl ~ ., 
                   data = data, 
                   method = "gbm",
                   trControl = fit_control)
  
  lpl.gbm.models[[i]] <- gbm_lpl
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}


### Random Forest ####

lpl.rf.models <- list()


for(i in 1:10) {
  
  
  data <- wd.lpl.imput.train.split %>% 
    filter(.imp == i) %>% 
    select(-.imp) %>% 
    as.data.frame()
  
  
  set.seed(1801)
  rf_lpl <- train(lpl ~ ., 
                  data = data, 
                  method = "ranger",
                  trControl = fit_control)
  
  lpl.rf.models[[i]] <- rf_lpl
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}




### Support vector machine ####

lpl.svm.models <- list()


for(i in 1:10) {
  
  
  data <- wd.lpl.imput.train.split %>% 
    filter(.imp == i) %>% 
    select(-.imp) %>% 
    as.data.frame()
  
  
  set.seed(1801)
  svm_lpl <- train(lpl ~ ., 
                   data = data, 
                   method = "svmRadialSigma",
                   trControl = fit_control)
  
  lpl.svm.models[[i]] <- svm_lpl
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}



stopCluster(cl)



##  Best model ####

# Best model decided based on the lowest prediction error


res.lpl <- data.frame()


for(i in 1:10) {
  
  res.lpl <- rbind(res.lpl, 
                   ModelEvaluation(models = lpl.tree.models,
                                   target.variable = "lpl",
                                   train.data = wd.lpl.imput.train.split,
                                   validation.data = wd.lpl.imput.val.split,
                                   model.name = "ctree",
                                   imputation = i)) %>%
    
    rbind(ModelEvaluation(models = lpl.gbm.models,
                          target.variable = "lpl",
                          train.data = wd.lpl.imput.train.split,
                          validation.data = wd.lpl.imput.val.split,
                          model.name = "gbm",
                          imputation = i)) %>%
    
    rbind(ModelEvaluation(models = lpl.rf.models,
                          target.variable = "lpl",
                          train.data = wd.lpl.imput.train.split,
                          validation.data = wd.lpl.imput.val.split,
                          model.name = "rf",
                          imputation = i)) %>% 
    
    rbind(ModelEvaluation(models = lpl.svm.models,
                          target.variable = "lpl",
                          train.data = wd.lpl.imput.train.split,
                          validation.data = wd.lpl.imput.val.split,
                          model.name = "svm",
                          imputation = i))
  
  
}


res.lpl %>% 
  filter(data.set == "validation") %>%
  group_by(model, data.set, metric) %>% 
  summarise(Mean = mean(value),
            SD = sd(value)) %>% 
  mutate_if(is.numeric, round, 3) %>% 
  # filter(metric %in% c("r2", "RMSE")) %>%
  arrange(desc(Mean))


# Random forest was the best model!


## Model interpretation ------------------------------------------------------

# Creating predictors

predictors.lpl <- list()

for(i in 1:10) {
  
  plan("callr", workers = 14)
  predictor <- Predictor$new(model = lpl.rf.models[[i]], 
                             data = wd.lpl.imputed.stats %>% 
                               filter(.imp == i) %>% 
                               select(-.imp),
                             y = "lpl")
  
  predictors.lpl[[i]] <- predictor
  
}




### Variable importance ####


var.imp.lpl <- data.frame()


for(i in 1:10) {
  
  
  plan("callr", workers = 14)
  
  set.seed(1801)
  imp.lpl <- FeatureImp$new(predictors.lpl[[i]], loss = "rmse",
                            compare = "ratio")
  
  var.imp.lpl <- imp.lpl$results %>% 
    mutate(imputation = i) %>% 
    rbind(var.imp.lpl)
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}


ggplot(var.imp.lpl %>%  
         group_by(feature) %>% 
         summarise(avg = mean(importance),
                   sd = sd(importance)) %>% 
         arrange(desc(avg)),
       aes(y = reorder(feature, + avg),
           x = avg)) +
  geom_point() +
  geom_errorbar(aes(xmin = avg - sd, xmax = avg + sd), 
                width = 0.2) +
  theme_classic(base_family = "Times New Roman") + 
  labs(x = "Variable importance") +
  theme_classic(base_family = "Times New Roman") +
  theme(axis.text = element_text(size = 10, color = "black"),
        axis.ticks.x = element_line(color = "black"),
        axis.ticks.y = element_line(NA),
        axis.line = element_line("black"),
        axis.title.y = element_blank(),
        axis.title.x = element_text(size = 10,
                                    margin = margin(t = 10, # top
                                                    r = 0, # right
                                                    b = 0, # bottom
                                                    l = 0)))



### Accumulated local effect ####

ale.lpl.res <- list()


for(i in 1:10) {
  
  
  plan("callr", workers = 14)
  
  set.seed(1801)
  ale.lpl <- FeatureEffects$new(predictors.lpl[[i]], method = "ale",
                                grid.size = 50)
  
  ale.lpl.res[[i]] <- ale.lpl
  
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}



ale.lpl.res.compiled <- data.frame()

for(i in 1:10){
  
  vars <- names(ale.lpl.res[[i]]$results)
  
  for(j in seq_along(vars)){
    
    ale.lpl.res.compiled <- ale.lpl.res[[i]]$results[j] %>%
      unname %>% 
      as.data.frame() %>% 
      mutate(imp = i) %>% 
      rbind(ale.lpl.res.compiled)
    
  }
}




# Percentage of 3rd or greater lactation cows ------------------------------------------------

# Exploratory graphs
wd.lact3plus.imputed.stats %>% 
  DataExplorer::plot_bar()

wd.lact3plus.imputed.stats %>% 
  DataExplorer::plot_histogram()

wd.lact3plus.imputed.stats %>% 
  DataExplorer::plot_correlation(type = "continuous")

wd.lact3plus.imputed.stats %>% 
  DataExplorer::plot_correlation(type = "discrete")



## Splitting data ####
wd.lact3plus.imput.train.split <- data.frame()
wd.lact3plus.imput.val.split <- data.frame()

for (i in 1:10) {
  
  data <- wd.lact3plus.imputed.stats %>%
    filter(.imp == i)
  
  splitIndex <- splitTools::partition(data$lact3plus,
                                      p = c(train = 0.80, valid = 0.20),
                                      seed = 1802)
  
  wd.lact3plus.imput.train.split <- rbind(wd.lact3plus.imput.train.split,
                                          data[splitIndex$train,])
  
  wd.lact3plus.imput.val.split <- rbind(wd.lact3plus.imput.val.split,
                                        data[splitIndex$valid,])
  
}


# Inspecting it
summary(wd.lact3plus.imput.train.split %>% 
          filter(.imp == 1))


summary(wd.lact3plus.imput.val.split %>% 
          filter(.imp == 1))



## Sanity check ####
# Training a classic mixed effect model for each of the imputed files


r2.lact3plus.lm <- c()
rmse.lact3plus.lm <- c()

for(i in 1:10) {
  
  lact3plus.lm <- lm(lact3plus ~ .,
                     data = wd.lact3plus.imput.train.split %>% 
                       filter(.imp == i) %>% 
                       select(-.imp))
  
  anova(lact3plus.lm) %>% print()
  
  
  r2.lact3plus.lm[i] <- caret::R2(pred = predict(lact3plus.lm,
                                                 wd.lact3plus.imput.val.split %>% 
                                                   filter(.imp == i)),
                                  obs = wd.lact3plus.imput.val.split %>% 
                                    filter(.imp == i) %>% 
                                    pull(lact3plus),
                                  formula = "corr")
  
  rmse.lact3plus.lm[i] <- caret::RMSE(pred = predict(lact3plus.lm,
                                                     wd.lact3plus.imput.val.split %>% 
                                                       filter(.imp == i)),
                                      obs = wd.lact3plus.imput.val.split %>% 
                                        filter(.imp == i) %>% 
                                        pull(lact3plus))
  
}


mean(r2.lact3plus.lm) %>% 
  round(3)

mean(rmse.lact3plus.lm) %>% 
  round(3)




## Training machine learning models ####

# Generating seeds for the random processes
number <- 5
repeats <- 10


set.seed(1801)
seeds <- vector(mode = "list", length = (number*repeats+1))
for(i in 1:(number*repeats)) seeds[[i]] <- sample.int(1000000, 27)

# For the last model:
seeds[[(number*repeats+1)]] <- sample.int(1000000, 1)



set.seed(1801)
fit_control <- trainControl(method = "adaptive_cv",
                            search = "grid",
                            number = number,
                            repeats = repeats,
                            adaptive = list(min = 5, alpha = 0.05, 
                                            method = "gls", 
                                            complete = TRUE),
                            allowParallel = TRUE,
                            verboseIter = FALSE,
                            seeds = seeds)


# Setting parallel clusters

cl <- makePSOCKcluster(parallel::detectCores())
registerDoParallel(cl, cores = parallel::detectCores())

getDoParWorkers()



### Classification and regression decision Tree ####

lact3plus.tree.models <- list()


for(i in 1:10) {
  
  
  data <- wd.lact3plus.imput.train.split %>% 
    filter(.imp == i) %>% 
    select(-.imp) %>% 
    as.data.frame()
  
  
  set.seed(1801)
  tree_lact3plus <- train(lact3plus ~ ., 
                          data = data, 
                          method = "rpart",
                          trControl = fit_control)
  
  lact3plus.tree.models[[i]] <- tree_lact3plus
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}



### Gradient boosting machine ####

lact3plus.gbm.models <- list()


for(i in 1:10) {
  
  
  data <- wd.lact3plus.imput.train.split %>% 
    filter(.imp == i) %>% 
    select(-.imp) %>% 
    as.data.frame()
  
  
  set.seed(1801)
  gbm_lact3plus <- train(lact3plus ~ ., 
                         data = data, 
                         method = "gbm",
                         trControl = fit_control)
  
  lact3plus.gbm.models[[i]] <- gbm_lact3plus
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}


### Random Forest ####

lact3plus.rf.models <- list()


for(i in 1:10) {
  
  
  data <- wd.lact3plus.imput.train.split %>% 
    filter(.imp == i) %>% 
    select(-.imp) %>% 
    as.data.frame()
  
  
  set.seed(1801)
  rf_lact3plus <- train(lact3plus ~ ., 
                        data = data, 
                        method = "ranger",
                        trControl = fit_control)
  
  lact3plus.rf.models[[i]] <- rf_lact3plus
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}




### Support vector machine ####

lact3plus.svm.models <- list()


for(i in 1:10) {
  
  
  data <- wd.lact3plus.imput.train.split %>% 
    filter(.imp == i) %>% 
    select(-.imp) %>% 
    as.data.frame()
  
  
  set.seed(1801)
  svm_lact3plus <- train(lact3plus ~ ., 
                         data = data, 
                         method = "svmRadialSigma",
                         trControl = fit_control)
  
  lact3plus.svm.models[[i]] <- svm_lact3plus
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}



stopCluster(cl)



##  Best model ####

# Best model decided based on the lowest prediction error


res.lact3plus <- data.frame()


for(i in 1:10) {
  
  res.lact3plus <- rbind(res.lact3plus, 
                         ModelEvaluation(models = lact3plus.tree.models,
                                         target.variable = "lact3plus",
                                         train.data = wd.lact3plus.imput.train.split,
                                         validation.data = wd.lact3plus.imput.val.split,
                                         model.name = "ctree",
                                         imputation = i)) %>%
    
    rbind(ModelEvaluation(models = lact3plus.gbm.models,
                          target.variable = "lact3plus",
                          train.data = wd.lact3plus.imput.train.split,
                          validation.data = wd.lact3plus.imput.val.split,
                          model.name = "gbm",
                          imputation = i)) %>%
    
    rbind(ModelEvaluation(models = lact3plus.rf.models,
                          target.variable = "lact3plus",
                          train.data = wd.lact3plus.imput.train.split,
                          validation.data = wd.lact3plus.imput.val.split,
                          model.name = "rf",
                          imputation = i)) %>% 
    
    rbind(ModelEvaluation(models = lact3plus.svm.models,
                          target.variable = "lact3plus",
                          train.data = wd.lact3plus.imput.train.split,
                          validation.data = wd.lact3plus.imput.val.split,
                          model.name = "svm",
                          imputation = i))
  
  
}


res.lact3plus %>% 
  filter(data.set == "validation") %>%
  group_by(model, data.set, metric) %>% 
  summarise(Mean = mean(value),
            SD = sd(value)) %>% 
  mutate_if(is.numeric, round, 3) %>% 
  # filter(metric %in% c("r2", "RMSE")) %>%
  arrange(desc(Mean))



# Gradient boosting machine was the best model!


## Model interpretation ------------------------------------------------------

# Creating predictors

predictors.lact3plus <- list()

for(i in 1:10) {
  
  plan("callr", workers = 14)
  predictor <- Predictor$new(model = lact3plus.gbm.models[[i]], 
                             data = wd.lact3plus.imputed.stats %>% 
                               filter(.imp == i) %>% 
                               select(-.imp),
                             y = "lact3plus")
  
  predictors.lact3plus[[i]] <- predictor
  
}




### Variable importance ####


var.imp.lact3plus <- data.frame()


for(i in 1:10) {
  
  
  plan("callr", workers = 14)
  
  set.seed(1801)
  imp.lact3plus <- FeatureImp$new(predictors.lact3plus[[i]], loss = "rmse",
                                  compare = "ratio")
  
  var.imp.lact3plus <- imp.lact3plus$results %>% 
    mutate(imputation = i) %>% 
    rbind(var.imp.lact3plus)
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}


# var.imp.lact3plus


ggplot(var.imp.lact3plus %>%  
         group_by(feature) %>% 
         summarise(avg = mean(importance),
                   sd = sd(importance)) %>% 
         arrange(desc(avg)),
       aes(y = reorder(feature, + avg),
           x = avg)) +
  geom_point() +
  geom_errorbar(aes(xmin = avg - sd, xmax = avg + sd), 
                width = 0.2) +
  theme_classic(base_family = "Times New Roman") + 
  labs(x = "Variable importance") +
  theme_classic(base_family = "Times New Roman") +
  theme(axis.text = element_text(size = 10, color = "black"),
        axis.ticks.x = element_line(color = "black"),
        axis.ticks.y = element_line(NA),
        axis.line = element_line("black"),
        axis.title.y = element_blank(),
        axis.title.x = element_text(size = 10,
                                    margin = margin(t = 10, # top
                                                    r = 0, # right
                                                    b = 0, # bottom
                                                    l = 0)))



### Accumulated local effect ####

ale.lact3plus.res <- list()


for(i in 1:10) {
  
  
  plan("callr", workers = 14)
  
  set.seed(1801)
  ale.lact3plus <- FeatureEffects$new(predictors.lact3plus[[i]], method = "ale",
                                      grid.size = 50)
  
  ale.lact3plus.res[[i]] <- ale.lact3plus
  
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}



ale.lact3plus.res.compiled <- data.frame()

for(i in 1:10){
  
  vars <- names(ale.lact3plus.res[[i]]$results)
  
  for(j in seq_along(vars)){
    
    ale.lact3plus.res.compiled <- ale.lact3plus.res[[i]]$results[j] %>%
      unname %>% 
      as.data.frame() %>% 
      mutate(imp = i) %>% 
      rbind(ale.lact3plus.res.compiled)
    
  }
}




# ECM  ------------------------------------------------

# Exploratory graphs
wd.ecm.imputed.stats %>% 
  DataExplorer::plot_bar()

wd.ecm.imputed.stats %>% 
  DataExplorer::plot_histogram()

wd.ecm.imputed.stats %>% 
  DataExplorer::plot_correlation(type = "continuous")

wd.ecm.imputed.stats %>% 
  DataExplorer::plot_correlation(type = "discrete")



## Splitting data ####
wd.ecm.imput.train.split <- data.frame()
wd.ecm.imput.val.split <- data.frame()

for (i in 1:10) {
  
  data <- wd.ecm.imputed.stats %>%
    filter(.imp == i)
  
  splitIndex <- splitTools::partition(data$ecm,
                                      p = c(train = 0.80, valid = 0.20),
                                      seed = 1801)
  
  wd.ecm.imput.train.split <- rbind(wd.ecm.imput.train.split,
                                    data[splitIndex$train,])
  
  wd.ecm.imput.val.split <- rbind(wd.ecm.imput.val.split,
                                  data[splitIndex$valid,])
  
}


# Inspecting it
summary(wd.ecm.imput.train.split %>% 
          filter(.imp == 1))


summary(wd.ecm.imput.val.split %>% 
          filter(.imp == 1))



## Sanity check ####
# Training a classic mixed effect model for each of the imputed files


r2.ecm.lm <- c()
rmse.ecm.lm <- c()

for(i in 1:10) {
  
  ecm.lm <- lm(ecm ~ .,
               data = wd.ecm.imput.train.split %>% 
                 filter(.imp == i) %>% 
                 select(-.imp))
  
  anova(ecm.lm) %>% print()
  
  
  r2.ecm.lm[i] <- caret::R2(pred = predict(ecm.lm,
                                           wd.ecm.imput.val.split %>% 
                                             filter(.imp == i)),
                            obs = wd.ecm.imput.val.split %>% 
                              filter(.imp == i) %>% 
                              pull(ecm),
                            formula = "corr")
  
  rmse.ecm.lm[i] <- caret::RMSE(pred = predict(ecm.lm,
                                               wd.ecm.imput.val.split %>% 
                                                 filter(.imp == i)),
                                obs = wd.ecm.imput.val.split %>% 
                                  filter(.imp == i) %>% 
                                  pull(ecm))
  
}


mean(r2.ecm.lm) %>% 
  round(2)

mean(rmse.ecm.lm) %>% 
  round(3)




## Training machine learning models ####

# Generating seeds for the random processes
number <- 5
repeats <- 10


set.seed(1801)
seeds <- vector(mode = "list", length = (number*repeats+1))
for(i in 1:(number*repeats)) seeds[[i]] <- sample.int(1000000, 27)

# For the last model:
seeds[[(number*repeats+1)]] <- sample.int(1000000, 1)



set.seed(1801)
fit_control <- trainControl(method = "adaptive_cv",
                            search = "grid",
                            number = number,
                            repeats = repeats,
                            adaptive = list(min = 5, alpha = 0.05, 
                                            method = "gls", 
                                            complete = TRUE),
                            allowParallel = TRUE,
                            verboseIter = FALSE,
                            seeds = seeds)

# Setting parallel clusters

cl <- makePSOCKcluster(parallel::detectCores())
registerDoParallel(cl, cores = parallel::detectCores())

getDoParWorkers()



### Classification and regression decision Tree ####

ecm.tree.models <- list()


for(i in 1:10) {
  
  
  data <- wd.ecm.imput.train.split %>% 
    filter(.imp == i) %>% 
    select(-.imp) %>% 
    as.data.frame()
  
  
  set.seed(1801)
  tree_ecm <- train(ecm ~ ., 
                    data = data, 
                    method = "rpart",
                    trControl = fit_control)
  
  ecm.tree.models[[i]] <- tree_ecm
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}



### Gradient boosting machine ####

ecm.gbm.models <- list()


for(i in 1:10) {
  
  
  data <- wd.ecm.imput.train.split %>% 
    filter(.imp == i) %>% 
    select(-.imp) %>% 
    as.data.frame()
  
  
  set.seed(1801)
  gbm_ecm <- train(ecm ~ ., 
                   data = data, 
                   method = "gbm",
                   trControl = fit_control)
  
  ecm.gbm.models[[i]] <- gbm_ecm
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}


### Random Forest ####

ecm.rf.models <- list()


for(i in 1:10) {
  
  
  data <- wd.ecm.imput.train.split %>% 
    filter(.imp == i) %>% 
    select(-.imp) %>% 
    as.data.frame()
  
  
  set.seed(1801)
  rf_ecm <- train(ecm ~ ., 
                  data = data, 
                  method = "ranger",
                  trControl = fit_control)
  
  ecm.rf.models[[i]] <- rf_ecm
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}




### Support vector machine ####

ecm.svm.models <- list()


for(i in 1:10) {
  
  
  data <- wd.ecm.imput.train.split %>% 
    filter(.imp == i) %>% 
    select(-.imp) %>% 
    as.data.frame()
  
  
  set.seed(1801)
  svm_ecm <- train(ecm ~ ., 
                   data = data, 
                   method = "svmRadialSigma",
                   trControl = fit_control)
  
  ecm.svm.models[[i]] <- svm_ecm
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}



stopCluster(cl)



##  Best model ####

# Best model decided based on the lowest prediction error


res.ecm <- data.frame()


for(i in 1:10) {
  
  res.ecm <- rbind(res.ecm, 
                   ModelEvaluation(models = ecm.tree.models,
                                   target.variable = "ecm",
                                   train.data = wd.ecm.imput.train.split,
                                   validation.data = wd.ecm.imput.val.split,
                                   model.name = "ctree",
                                   imputation = i)) %>%
    
    rbind(ModelEvaluation(models = ecm.gbm.models,
                          target.variable = "ecm",
                          train.data = wd.ecm.imput.train.split,
                          validation.data = wd.ecm.imput.val.split,
                          model.name = "gbm",
                          imputation = i)) %>%
    
    rbind(ModelEvaluation(models = ecm.rf.models,
                          target.variable = "ecm",
                          train.data = wd.ecm.imput.train.split,
                          validation.data = wd.ecm.imput.val.split,
                          model.name = "rf",
                          imputation = i)) %>% 
    
    rbind(ModelEvaluation(models = ecm.svm.models,
                          target.variable = "ecm",
                          train.data = wd.ecm.imput.train.split,
                          validation.data = wd.ecm.imput.val.split,
                          model.name = "svm",
                          imputation = i))
  
  
}


res.ecm %>% 
  filter(data.set == "validation") %>%
  group_by(model, data.set, metric) %>% 
  summarise(Mean = mean(value),
            SD = sd(value)) %>% 
  mutate_if(is.numeric, round, 3) %>% 
  # filter(metric %in% c("r2", "RMSE")) %>%
  arrange(desc(Mean))


# Gradient boosting machine was the best model!


## Model interpretation ------------------------------------------------------

# Creating predictors

predictors.ecm <- list()

for(i in 1:10) {
  
  plan("callr", workers = 14)
  predictor <- Predictor$new(model = ecm.gbm.models[[i]],
                             data = wd.ecm.imputed.stats %>%
                               filter(.imp == i) %>%
                               select(-.imp),
                             y = "ecm")
  
  predictors.ecm[[i]] <- predictor
  
}




### Variable importance ####


var.imp.ecm <- data.frame()


for(i in 1:10) {
  
  
  plan("callr", workers = 14)
  
  set.seed(1801)
  imp.ecm <- FeatureImp$new(predictors.ecm[[i]], loss = "rmse",
                            compare = "ratio")
  
  var.imp.ecm <- imp.ecm$results %>%
    mutate(imputation = i) %>%
    rbind(var.imp.ecm)
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}


# var.imp.ecm


ggplot(var.imp.ecm %>%
         group_by(feature) %>%
         summarise(avg = mean(importance),
                   sd = sd(importance)) %>%
         arrange(desc(avg)),
       aes(y = reorder(feature, + avg),
           x = avg)) +
  geom_point() +
  geom_errorbar(aes(xmin = avg - sd, xmax = avg + sd),
                width = 0.2) +
  theme_classic(base_family = "Times New Roman") +
  labs(x = "Variable importance") +
  theme_classic(base_family = "Times New Roman") +
  theme(axis.text = element_text(size = 10, color = "black"),
        axis.ticks.x = element_line(color = "black"),
        axis.ticks.y = element_line(NA),
        axis.line = element_line("black"),
        axis.title.y = element_blank(),
        axis.title.x = element_text(size = 10,
                                    margin = margin(t = 10, # top
                                                    r = 0, # right
                                                    b = 0, # bottom
                                                    l = 0)))



### Accumulated local effect ####

ale.ecm.res <- list()


for(i in 1:10) {
  
  
  plan("callr", workers = 14)
  
  set.seed(1801)
  ale.ecm <- FeatureEffects$new(predictors.ecm[[i]], method = "ale",
                                grid.size = 50)
  
  ale.ecm.res[[i]] <- ale.ecm
  
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}


# ale.ecm.res[[1]] %>%
#   plot()



ale.ecm.res.compiled <- data.frame()

for(i in 1:10){
  
  vars <- names(ale.ecm.res[[i]]$results)
  
  for(j in seq_along(vars)){
    
    ale.ecm.res.compiled <- ale.ecm.res[[i]]$results[j] %>%
      unname %>%
      as.data.frame() %>%
      mutate(imp = i) %>%
      rbind(ale.ecm.res.compiled)
    
  }
}


# Milk value  ------------------------------------------------

# Exploratory graphs
wd.cumul_milk_value.imputed.stats %>% 
  DataExplorer::plot_bar()

wd.cumul_milk_value.imputed.stats %>% 
  DataExplorer::plot_histogram()

wd.cumul_milk_value.imputed.stats %>% 
  DataExplorer::plot_correlation(type = "continuous")

wd.cumul_milk_value.imputed.stats %>% 
  DataExplorer::plot_correlation(type = "discrete")



## Splitting data ####
wd.cumul_milk_value.imput.train.split <- data.frame()
wd.cumul_milk_value.imput.val.split <- data.frame()

for (i in 1:10) {
  
  data <- wd.cumul_milk_value.imputed.stats %>%
    filter(.imp == i)
  
  splitIndex <- splitTools::partition(data$cumul_milk_value,
                                      p = c(train = 0.80, valid = 0.20),
                                      seed = 1801)
  
  wd.cumul_milk_value.imput.train.split <- rbind(wd.cumul_milk_value.imput.train.split,
                                                 data[splitIndex$train,])
  
  wd.cumul_milk_value.imput.val.split <- rbind(wd.cumul_milk_value.imput.val.split,
                                               data[splitIndex$valid,])
  
}


# Inspecting it
summary(wd.cumul_milk_value.imput.train.split %>% 
          filter(.imp == 1))


summary(wd.cumul_milk_value.imput.val.split %>% 
          filter(.imp == 1))



## Sanity check ####
# Training a classic mixed effect model for each of the imputed files


r2.cumul_milk_value.lm <- c()
rmse.cumul_milk_value.lm <- c()

for(i in 1:10) {
  
  cumul_milk_value.lm <- lm(cumul_milk_value ~ .,
                            data = wd.cumul_milk_value.imput.train.split %>% 
                              filter(.imp == i) %>% 
                              select(-.imp))
  
  anova(cumul_milk_value.lm) %>% print()
  
  
  r2.cumul_milk_value.lm[i] <- caret::R2(pred = predict(cumul_milk_value.lm,
                                                        wd.cumul_milk_value.imput.val.split %>% 
                                                          filter(.imp == i)),
                                         obs = wd.cumul_milk_value.imput.val.split %>% 
                                           filter(.imp == i) %>% 
                                           pull(cumul_milk_value),
                                         formula = "corr")
  
  rmse.cumul_milk_value.lm[i] <- caret::RMSE(pred = predict(cumul_milk_value.lm,
                                                            wd.cumul_milk_value.imput.val.split %>% 
                                                              filter(.imp == i)),
                                             obs = wd.cumul_milk_value.imput.val.split %>% 
                                               filter(.imp == i) %>% 
                                               pull(cumul_milk_value))
  
}


mean(r2.cumul_milk_value.lm) %>% 
  round(2)

mean(rmse.cumul_milk_value.lm) %>% 
  round(3)




## Training machine learning models ####

# Generating seeds for the random processes
number <- 5
repeats <- 10


set.seed(1801)
seeds <- vector(mode = "list", length = (number*repeats+1))
for(i in 1:(number*repeats)) seeds[[i]] <- sample.int(1000000, 27)

# For the last model:
seeds[[(number*repeats+1)]] <- sample.int(1000000, 1)



set.seed(1801)
fit_control <- trainControl(method = "adaptive_cv",
                            search = "grid",
                            number = number,
                            repeats = repeats,
                            adaptive = list(min = 5, alpha = 0.05, 
                                            method = "gls", 
                                            complete = TRUE),
                            allowParallel = TRUE,
                            verboseIter = FALSE,
                            seeds = seeds)


# Setting parallel clusters

cl <- makePSOCKcluster(parallel::detectCores())
registerDoParallel(cl, cores = parallel::detectCores())

getDoParWorkers()



### Classification and regression decision Tree ####

cumul_milk_value.tree.models <- list()


for(i in 1:10) {
  
  
  data <- wd.cumul_milk_value.imput.train.split %>% 
    filter(.imp == i) %>% 
    select(-.imp) %>% 
    as.data.frame()
  
  
  set.seed(1801)
  tree_cumul_milk_value <- train(cumul_milk_value ~ ., 
                                 data = data, 
                                 method = "rpart",
                                 trControl = fit_control)
  
  cumul_milk_value.tree.models[[i]] <- tree_cumul_milk_value
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}



### Gradient boosting machine ####

cumul_milk_value.gbm.models <- list()


for(i in 1:10) {
  
  
  data <- wd.cumul_milk_value.imput.train.split %>% 
    filter(.imp == i) %>% 
    select(-.imp) %>% 
    as.data.frame()
  
  
  set.seed(1801)
  gbm_cumul_milk_value <- train(cumul_milk_value ~ ., 
                                data = data, 
                                method = "gbm",
                                trControl = fit_control)
  
  cumul_milk_value.gbm.models[[i]] <- gbm_cumul_milk_value
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}


### Random Forest ####

cumul_milk_value.rf.models <- list()


for(i in 1:10) {
  
  
  data <- wd.cumul_milk_value.imput.train.split %>% 
    filter(.imp == i) %>% 
    select(-.imp) %>% 
    as.data.frame()
  
  
  set.seed(1801)
  rf_cumul_milk_value <- train(cumul_milk_value ~ ., 
                               data = data, 
                               method = "ranger",
                               trControl = fit_control)
  
  cumul_milk_value.rf.models[[i]] <- rf_cumul_milk_value
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}




### Support vector machine ####

cumul_milk_value.svm.models <- list()


for(i in 1:10) {
  
  
  data <- wd.cumul_milk_value.imput.train.split %>% 
    filter(.imp == i) %>% 
    select(-.imp) %>% 
    as.data.frame()
  
  
  set.seed(1801)
  svm_cumul_milk_value <- train(cumul_milk_value ~ ., 
                                data = data, 
                                method = "svmRadialSigma",
                                trControl = fit_control)
  
  cumul_milk_value.svm.models[[i]] <- svm_cumul_milk_value
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}



stopCluster(cl)



##  Best model ####

# Best model decided based on the lowest prediction error


res.cumul_milk_value <- data.frame()


for(i in 1:10) {
  
  res.cumul_milk_value <- rbind(res.cumul_milk_value, 
                                ModelEvaluation(models = cumul_milk_value.tree.models,
                                                target.variable = "cumul_milk_value",
                                                train.data = wd.cumul_milk_value.imput.train.split,
                                                validation.data = wd.cumul_milk_value.imput.val.split,
                                                model.name = "ctree",
                                                imputation = i)) %>%
    
    rbind(ModelEvaluation(models = cumul_milk_value.gbm.models,
                          target.variable = "cumul_milk_value",
                          train.data = wd.cumul_milk_value.imput.train.split,
                          validation.data = wd.cumul_milk_value.imput.val.split,
                          model.name = "gbm",
                          imputation = i)) %>%
    
    rbind(ModelEvaluation(models = cumul_milk_value.rf.models,
                          target.variable = "cumul_milk_value",
                          train.data = wd.cumul_milk_value.imput.train.split,
                          validation.data = wd.cumul_milk_value.imput.val.split,
                          model.name = "rf",
                          imputation = i)) %>% 
    
    rbind(ModelEvaluation(models = cumul_milk_value.svm.models,
                          target.variable = "cumul_milk_value",
                          train.data = wd.cumul_milk_value.imput.train.split,
                          validation.data = wd.cumul_milk_value.imput.val.split,
                          model.name = "svm",
                          imputation = i))
  
  
}


res.cumul_milk_value %>% 
  filter(data.set == "validation") %>%
  group_by(model, data.set, metric) %>% 
  summarise(Mean = mean(value),
            SD = sd(value)) %>% 
  data.frame() %>% 
  mutate_if(is.numeric, round, 3) %>% 
  # filter(metric %in% c("r2", "RMSE")) %>%
  arrange(desc(Mean))



res.cumul_milk_value %>% 
  filter(data.set == "validation") %>%
  group_by(model, data.set, metric) %>% 
  summarise(Mean = mean(value),
            SD = sd(value)) %>% 
  filter(metric != "r2") %>% 
  group_by(metric) %>% 
  slice_min(Mean)

res.cumul_milk_value %>% 
  filter(data.set == "validation") %>%
  group_by(model, data.set, metric) %>% 
  summarise(Mean = mean(value),
            SD = sd(value)) %>% 
  filter(metric == "r2") %>% 
  group_by(metric) %>% 
  slice_max(Mean)



# Gradient boosting machine was the best model!


## Model interpretation ------------------------------------------------------

# Creating predictors

predictors.cumul_milk_value <- list()

for(i in 1:10) {
  
  plan("callr", workers = 14)
  predictor <- Predictor$new(model = cumul_milk_value.gbm.models[[i]],
                             data = wd.cumul_milk_value.imputed.stats %>%
                               filter(.imp == i) %>%
                               select(-.imp),
                             y = "cumul_milk_value")
  
  predictors.cumul_milk_value[[i]] <- predictor
  
}




### Variable importance ####


var.imp.cumul_milk_value <- data.frame()


for(i in 1:10) {
  
  
  plan("callr", workers = 14)
  
  set.seed(1801)
  imp.cumul_milk_value <- FeatureImp$new(predictors.cumul_milk_value[[i]], loss = "rmse",
                                         compare = "ratio")
  
  var.imp.cumul_milk_value <- imp.cumul_milk_value$results %>%
    mutate(imputation = i) %>%
    rbind(var.imp.cumul_milk_value)
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}


# var.imp.cumul_milk_value


ggplot(var.imp.cumul_milk_value %>%
         group_by(feature) %>%
         summarise(avg = mean(importance),
                   sd = sd(importance)) %>%
         arrange(desc(avg)),
       aes(y = reorder(feature, + avg),
           x = avg)) +
  geom_point() +
  geom_errorbar(aes(xmin = avg - sd, xmax = avg + sd),
                width = 0.2) +
  theme_classic(base_family = "Times New Roman") +
  labs(x = "Variable importance") +
  theme_classic(base_family = "Times New Roman") +
  theme(axis.text = element_text(size = 10, color = "black"),
        axis.ticks.x = element_line(color = "black"),
        axis.ticks.y = element_line(NA),
        axis.line = element_line("black"),
        axis.title.y = element_blank(),
        axis.title.x = element_text(size = 10,
                                    margin = margin(t = 10, # top
                                                    r = 0, # right
                                                    b = 0, # bottom
                                                    l = 0)))



### Accumulated local effect ####

ale.cumul_milk_value.res <- list()


for(i in 1:10) {
  
  
  plan("callr", workers = 14)
  
  set.seed(1801)
  ale.cumul_milk_value <- FeatureEffects$new(predictors.cumul_milk_value[[i]], method = "ale",
                                             grid.size = 50)
  
  ale.cumul_milk_value.res[[i]] <- ale.cumul_milk_value
  
  
  print(
    paste(i, " imputed data set finished. Only ", 10-i, " sets missing :)",
          sep = "")
  )
  
  
}



ale.cumul_milk_value.res.compiled <- data.frame()

for(i in 1:10){
  
  vars <- names(ale.cumul_milk_value.res[[i]]$results)
  
  for(j in seq_along(vars)){
    
    ale.cumul_milk_value.res.compiled <- ale.cumul_milk_value.res[[i]]$results[j] %>%
      unname %>%
      as.data.frame() %>%
      mutate(imp = i) %>%
      rbind(ale.cumul_milk_value.res.compiled)
    
  }
}




# Final results -------------------------------------------------------------

## Combined model performance ####
performance.res <- res.lpl %>% 
  mutate(outcome = "lpl") %>% 
  
  rbind(res.lact3plus %>% 
          mutate(outcome = "lact3plus")) %>%
  
  rbind(res.ecm %>% 
          mutate(outcome = "ecm")) %>% 
  
  rbind(res.cumul_milk_value %>% 
          mutate(outcome = "mv")) %>% 
  
  
  group_by(outcome, model, data.set, metric) %>% 
  summarise(Mean = mean(value),
            SD = sd(value)) %>% 
  ungroup() %>% 
  mutate(outcome = factor(outcome,
                          levels = c("lpl","lact3plus", "ecm", "mv")),
         metric = factor(metric,
                         levels = c("r2", "RMSE", "MAE", "MAAPE")),
         model = factor(model,
                        levels = c("ctree", "gbm", "rf", "svm")))



performance.res %>% 
  filter(data.set == "training") %>% 
  data.frame() %>% 
  plyr::arrange(outcome, model, metric) %>% 
  mutate_at("Mean", round, 1) %>% 
  mutate_at("SD", round, 4) %>% 
  rhandsontable::rhandsontable(useTypes = FALSE)


performance.res %>% 
  filter(data.set == "validation") %>%
  data.frame() %>% 
  plyr::arrange(outcome, model, metric) %>% 
  mutate_at("Mean", round, 1) %>% 
  mutate_at("SD", round, 2) %>% 
  rhandsontable::rhandsontable(useTypes = FALSE)




## Combined variable importance ####



## Variable importance ####

var.imp.res <- var.imp.lpl %>% 
  mutate(outcome = "lpl") %>% 
  
  rbind(var.imp.lact3plus %>% 
          mutate(outcome = "lact3plus")) %>%
  
  rbind(var.imp.ecm %>% 
          mutate(outcome = "ecm")) %>% 
  
  rbind(var.imp.cumul_milk_value %>% 
          mutate(outcome = "mv")) %>% 
  
  group_by(outcome, feature) %>% 
  summarise(avg = mean(importance),
            sd = sd(importance)) %>%  
  
  mutate(feature = dplyr::recode(feature, 
                                 
                                 bdng_added_fqcy = "Frequency bedding addition",
                                 calf_milk_feed_sys = "Milk feeding system",
                                 calf_milk_source_forms = "Milk state",
                                 calf_milk_sources = "Milk source",
                                 colostrum_sys = "Colostrum feeding system",
                                 daily_feeding_amnt = "Daily feeding amount",
                                 first_concentrate_age_days = "Age concentrate first offered",
                                 first_water_age_days = "Age water first offered",
                                 igg_conc_method = "Colostrum IgG assessment",
                                 milk_replacer_fat_pcnt = "Milk replacer fat",
                                 milk_replacer_prt_pcnt = "Milk replacer protein",
                                 milk_replacer_medicated_ind = "Milk replacer medicated",
                                 noweaned_housing_details = "Non-weaned housing detail",
                                 noweaned_housing_groupings = "Non-weaned housing",
                                 starter_feed_prot_pcnts = "Starter protein",
                                 weaned_housing_details = "Weaned housing detail",
                                 weaned_housing_groupings = "Weaned housing"))



varImp.plots <- plot_grid(
  
  ggplot(var.imp.res %>%
           filter(outcome == "lpl") %>%
           arrange(desc(avg)),
         
         aes(y = reorder(feature, + avg),
             x = avg)) +
    geom_point(size = 0.9) +
    geom_errorbar(aes(xmin = avg - sd, xmax = avg + sd), 
                  width = 0.2) +
    theme_classic(base_family = "Times New Roman") + 
    labs(x = "Variable importance") +
    theme_classic(base_family = "Times New Roman") +
    theme(axis.text = element_text(size = 10, color = "black"),
          axis.ticks.x = element_line(color = "black"),
          axis.ticks.y = element_line(NA),
          axis.line = element_line("black"),
          axis.title.y = element_blank(),
          axis.title.x = element_text(size = 10,
                                      margin = margin(t = 10, # top
                                                      r = 0, # right
                                                      b = 0, # bottom
                                                      l = 0))),
  
  ggplot(var.imp.res %>%
           filter(outcome == "lact3plus") %>%
           arrange(desc(avg)),
         aes(y = reorder(feature, + avg),
             x = avg)) +
    geom_point(size = 0.9) +
    geom_errorbar(aes(xmin = avg - sd, xmax = avg + sd), 
                  width = 0.2) +
    theme_classic(base_family = "Times New Roman") +  
    labs(x = "Variable importance") +
    theme_classic(base_family = "Times New Roman") +
    theme(axis.text = element_text(size = 10, color = "black"),
          axis.ticks.x = element_line(color = "black"),
          axis.ticks.y = element_line(NA),
          axis.line = element_line("black"),
          axis.title.y = element_blank(),
          axis.title.x = element_text(size = 10,
                                      margin = margin(t = 10, # top
                                                      r = 0, # right
                                                      b = 0, # bottom
                                                      l = 0))),
  
  ggplot(var.imp.res %>%
           filter(outcome == "ecm") %>%
           arrange(desc(avg)),
         aes(y = reorder(feature, + avg),
             x = avg)) +
    geom_point(size = 0.9) +
    geom_errorbar(aes(xmin = avg - sd, xmax = avg + sd), 
                  width = 0.2) +
    theme_classic(base_family = "Times New Roman") +  
    labs(x = "Variable importance") +
    theme_classic(base_family = "Times New Roman") +
    theme(axis.text = element_text(size = 10, color = "black"),
          axis.ticks.x = element_line(color = "black"),
          axis.ticks.y = element_line(NA),
          axis.line = element_line("black"),
          axis.title.y = element_blank(),
          axis.title.x = element_text(size = 10,
                                      margin = margin(t = 10, # top
                                                      r = 0, # right
                                                      b = 0, # bottom
                                                      l = 0))),
  
  ggplot(var.imp.res %>%
           filter(outcome == "mv") %>%
           arrange(desc(avg)),
         aes(y = reorder(feature, + avg),
             x = avg)) +
    geom_point(size = 0.9) +
    geom_errorbar(aes(xmin = avg - sd, xmax = avg + sd), 
                  width = 0.2) +
    theme_classic(base_family = "Times New Roman") + 
    labs(x = "Variable importance") +
    theme_classic(base_family = "Times New Roman") +
    theme(axis.text = element_text(size = 10, color = "black"),
          axis.ticks.x = element_line(color = "black"),
          axis.ticks.y = element_line(NA),
          axis.line = element_line("black"),
          axis.title.y = element_blank(),
          axis.title.x = element_text(size = 10,
                                      margin = margin(t = 10, # top
                                                      r = 0, # right
                                                      b = 0, # bottom
                                                      l = 0))),
  
  
  label_fontfamily = "Times New Roman",
  nrow = 2,
  labels = c("A", "B", "C", "D"),#, "E"),
  label_size = 14,
  scale = rep(0.97, 4))


varImp.plots



## Acumulated local effect ####

ale.res.compiled <- ale.lpl.res.compiled %>% 
  mutate(outcome = "lpl") %>% 
  rbind(ale.lact3plus.res.compiled %>% 
          mutate(outcome = "lact3plus")) %>% 
  rbind(ale.ecm.res.compiled %>% 
          mutate(outcome = "ecm")) %>% 
  rbind(ale.cumul_milk_value.res.compiled %>% 
          mutate(outcome = "mv")) %>% 
ungroup() %>% 
  
  mutate(outcome = factor(outcome,
                          levels = c("lpl", "lact3plus", "ecm", "mv")),
         .feature = factor(.feature,
                           levels = c("colostrum_sys", "igg_conc_method",
                                      "calf_milk_sources", "calf_milk_source_forms", 
                                      "calf_milk_feed_sys", "milk_replacer_medicated_ind",
                                      "noweaned_housing_details", "noweaned_housing_groupings",
                                      "weaned_housing_details", "weaned_housing_groupings",
                                      
                                      # Numeric variables
                                      "first_concentrate_age_days", "bdng_added_fqcy", "first_water_age_days", 
                                      "starter_feed_prot_pcnts", "daily_feeding_amnt", "milk_replacer_fat_pcnt", 
                                      "milk_replacer_prt_pcnt")))



ale.res.compiled %>% 
  filter(.feature == "igg_conc_method") %>% 
  group_by(outcome, .feature, .borders) %>% 
  summarise(AVG = mean(.value),
            SD = sd(.value)) %>% 
  data.frame() %>% 
  mutate_at("AVG", round, 1) %>%
  mutate_at("SD", round, 2) %>%
  rhandsontable::rhandsontable(useTypes = FALSE)




#### 3rd or greater lactation ####



plot_grid(
  
  ggplot(ale.res.compiled %>%
           filter(outcome == "lact3plus", .feature == "daily_feeding_amnt") %>%
           mutate(.borders = as.numeric(.borders)),
         aes(x = .borders,
             y = .value)) +
    geom_line(aes(group = imp),
              colour = "grey") +
    geom_smooth(se = F)+
    scale_x_continuous(expand = expansion(mult = c(0.01, 0.01))) +
    theme_classic(base_family = "Times New Roman") +
    labs(x = "Daily feeding (L)",
         y = "Accumulated local effect") +
    theme_classic(base_family = "Times New Roman") +
    theme(axis.text = element_text(size = 12, color = "black"),
          axis.ticks = element_line(color = "black"),
          axis.line = element_line("black"),
          axis.title.y = element_text(size = 12,
                                      margin = margin(t = 0, # top
                                                      r = 10, # right
                                                      b = 0, # bottom
                                                      l = 0)),
          axis.title.x = element_text(size = 12,
                                      margin = margin(t = 10, # top
                                                      r = 0, # right
                                                      b = 0, # bottom
                                                      l = 0))),
  
  
  ggplot(ale.res.compiled %>%
           filter(outcome == "lact3plus", .feature == "first_concentrate_age_days") %>%
           mutate(.borders = as.numeric(.borders)),
         aes(x = .borders,
             y = .value)) +
    geom_line(aes(group = imp),
              colour = "grey") +
    geom_smooth(se = F)+
    theme_classic(base_family = "Times New Roman") +
    scale_x_continuous(expand = expansion(mult = c(0.01, 0.01))) +
    labs(x = "Age concentrate feeding first offered (day)",
         y = "Accumulated local effect") +
    theme_classic(base_family = "Times New Roman") +
    theme(axis.text = element_text(size = 12, color = "black"),
          axis.ticks = element_line(color = "black"),
          axis.line = element_line("black"),
          axis.title.y = element_text(size = 12,
                                      margin = margin(t = 0, # top
                                                      r = 10, # right
                                                      b = 0, # bottom
                                                      l = 0)),
          axis.title.x = element_text(size = 12,
                                      margin = margin(t = 10, # top
                                                      r = 0, # right
                                                      b = 0, # bottom
                                                      l = 0))),
  
  
  ggplot(ale.res.compiled %>% 
           filter(outcome == "lact3plus", .feature == "first_water_age_days") %>% 
           mutate(.borders = as.numeric(.borders)),
         aes(x = .borders,
             y = .value)) +
    geom_line(aes(group = imp),
              colour = "grey") +
    geom_smooth(se = F)+
    theme_classic(base_family = "Times New Roman") + 
    scale_x_continuous(expand = expansion(mult = c(0.01, 0.01))) +
    labs(x = "Age water is first offered (day)",
         y = "Accumulated local effect") +
    theme_classic(base_family = "Times New Roman") +
    theme(axis.text = element_text(size = 12, color = "black"),
          axis.ticks = element_line(color = "black"),
          axis.line = element_line("black"),
          axis.title.y = element_text(size = 12,
                                      margin = margin(t = 0, # top
                                                      r = 10, # right
                                                      b = 0, # bottom
                                                      l = 0)),
          axis.title.x = element_text(size = 12,
                                      margin = margin(t = 10, # top
                                                      r = 0, # right
                                                      b = 0, # bottom
                                                      l = 0))),
  
  
  
  label_fontfamily = "Times New Roman",
  nrow = 1,
  labels = c("A", "B", "C"),
  label_size = 14,
  scale = rep(0.98, 3))



#### Energy-corrected milk ####

plot_grid(
  
  ggplot(ale.res.compiled %>% 
           filter(outcome == "ecm", .feature == "first_concentrate_age_days") %>% 
           mutate(.borders = as.numeric(.borders)),
         aes(x = .borders,
             y = .value)) +
    geom_line(aes(group = imp),
              colour = "grey") +
    geom_smooth(se = F)+
    theme_classic(base_family = "Times New Roman") + 
    scale_x_continuous(expand = expansion(mult = c(0.01, 0.01))) +
    labs(x = "Age concentrate feeding first offered (day)",
         y = "Accumulated local effect") +
    theme_classic(base_family = "Times New Roman") +
    theme(axis.text = element_text(size = 12, color = "black"),
          axis.ticks = element_line(color = "black"),
          axis.line = element_line("black"),
          axis.title.y = element_text(size = 12,
                                      margin = margin(t = 0, # top
                                                      r = 10, # right
                                                      b = 0, # bottom
                                                      l = 0)),
          axis.title.x = element_text(size = 12,
                                      margin = margin(t = 10, # top
                                                      r = 0, # right
                                                      b = 0, # bottom
                                                      l = 0))),
  
  
  ggplot(ale.res.compiled %>% 
           filter(outcome == "ecm", .feature == "milk_replacer_prt_pcnt") %>% 
           mutate(.borders = as.numeric(.borders)),
         aes(x = .borders,
             y = .value)) +
    geom_line(aes(group = imp),
              colour = "grey") +
    geom_smooth(se = F)+
    theme_classic(base_family = "Times New Roman") + 
    scale_x_continuous(expand = expansion(mult = c(0.01, 0.01))) +
    scale_y_continuous(breaks = seq(-50, 150, by = 50),
                       limits = c(-65, 170)) +
    labs(x = "Milk replacer protein (%)",
         y = "Accumulated local effect") +
    theme_classic(base_family = "Times New Roman") +
    theme(axis.text = element_text(size = 12, color = "black"),
          axis.ticks = element_line(color = "black"),
          axis.line = element_line("black"),
          axis.title.y = element_text(size = 12,
                                      margin = margin(t = 0, # top
                                                      r = 10, # right
                                                      b = 0, # bottom
                                                      l = 0)),
          axis.title.x = element_text(size = 12,
                                      margin = margin(t = 10, # top
                                                      r = 0, # right
                                                      b = 0, # bottom
                                                      l = 0))),
  
  
  ggplot(ale.res.compiled %>% 
           filter(outcome == "ecm", .feature == "daily_feeding_amnt") %>% 
           mutate(.borders = as.numeric(.borders)),
         aes(x = .borders,
             y = .value)) +
    geom_line(aes(group = imp),
              colour = "grey") +
    geom_smooth(se = F)+
    theme_classic(base_family = "Times New Roman") + 
    scale_x_continuous(expand = expansion(mult = c(0.01, 0.01))) + 
    scale_y_continuous(limits = c(-210, 100)) +
    labs(x = "Daily feeding (L)",
         y = "Accumulated local effect") +
    theme_classic(base_family = "Times New Roman") +
    theme(axis.text = element_text(size = 12, color = "black"),
          axis.ticks = element_line(color = "black"),
          axis.line = element_line("black"),
          axis.title.y = element_text(size = 12,
                                      margin = margin(t = 0, # top
                                                      r = 10, # right
                                                      b = 0, # bottom
                                                      l = 0)),
          axis.title.x = element_text(size = 12,
                                      margin = margin(t = 10, # top
                                                      r = 0, # right
                                                      b = 0, # bottom
                                                      l = 0))),
  
  
  label_fontfamily = "Times New Roman",
  nrow = 1,
  labels = c("A", "B", "C"),
  label_size = 14,
  scale = rep(0.98, 3))



### Milk value ####

plot_grid(
  
  ggplot(ale.res.compiled %>% 
           filter(outcome == "mv", .feature == "first_concentrate_age_days") %>% 
           mutate(.borders = as.numeric(.borders)),
         aes(x = .borders,
             y = .value)) +
    geom_line(aes(group = imp),
              colour = "grey") +
    geom_smooth(se = F)+
    theme_classic(base_family = "Times New Roman") + 
    scale_x_continuous(expand = expansion(mult = c(0.01, 0.01))) +
    labs(x = "Age concentrate feeding first offered (day)",
         y = "Accumulated local effect") +
    theme_classic(base_family = "Times New Roman") +
    theme(axis.text = element_text(size = 10, color = "black"),
          axis.ticks = element_line(color = "black"),
          axis.line = element_line("black"),
          axis.title.y = element_text(size = 10,
                                      margin = margin(t = 0, # top
                                                      r = 10, # right
                                                      b = 0, # bottom
                                                      l = 0)),
          axis.title.x = element_text(size = 10,
                                      margin = margin(t = 10, # top
                                                      r = 0, # right
                                                      b = 0, # bottom
                                                      l = 0))),
  
  ggplot(ale.res.compiled %>% 
           filter(outcome == "mv", .feature == "milk_replacer_prt_pcnt") %>% 
           mutate(.borders = as.numeric(.borders)),
         aes(x = .borders,
             y = .value)) +
    geom_line(aes(group = imp),
              colour = "grey") +
    geom_smooth(se = F)+
    theme_classic(base_family = "Times New Roman") + 
    scale_x_continuous(expand = expansion(mult = c(0.01, 0.01))) +
    labs(x = "Milk replacer protein (%)",
         y = "Accumulated local effect") +
    theme_classic(base_family = "Times New Roman") +
    theme(axis.text = element_text(size = 10, color = "black"),
          axis.ticks = element_line(color = "black"),
          axis.line = element_line("black"),
          axis.title.y = element_text(size = 10,
                                      margin = margin(t = 0, # top
                                                      r = 10, # right
                                                      b = 0, # bottom
                                                      l = 0)),
          axis.title.x = element_text(size = 10,
                                      margin = margin(t = 10, # top
                                                      r = 0, # right
                                                      b = 0, # bottom
                                                      l = 0))),
  
  
  label_fontfamily = "Times New Roman",
  nrow = 1,
  labels = c("A", "B"),
  label_size = 14,
  scale = rep(0.98, 2))


